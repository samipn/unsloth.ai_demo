{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/samipn/unsloth.ai_demo/blob/main/colab5_continued_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35714f",
   "metadata": {
    "id": "dc35714f"
   },
   "source": [
    "# Colab 5 ‚Äî Continued Pretraining (teach a new language) + resume/export\n",
    "Show CPT on raw text to extend vocabulary/knowledge, then resume from a LoRA/adapter checkpoint and export to Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55399c62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55399c62",
    "outputId": "a9e8a596-4069-4b25-8b55-a4ee2146cc58"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mPyTorch: 2.8.0+cu126 CUDA: 12.6 Python: 3.12.12\n"
     ]
    }
   ],
   "source": [
    "#@title Install Unsloth + deps (Colab-safe)\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install unsloth datasets trl transformers accelerate bitsandbytes peft --no-cache-dir\n",
    "import torch, platform\n",
    "print(\"PyTorch:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"Python:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fde15a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458,
     "referenced_widgets": [
      "37b9ab9478404979928dcc71c1f60c8d",
      "64101051f1ec47638d620c910262bdef",
      "595d06e909a24459ac4ab0561b93efca",
      "e9449026cf044e38bd60540becbda0ef",
      "06eb56bf845242148de78ab51466a567",
      "5fdb3fdb36b54e7083295e48c0f03a93",
      "d7694734e62b40178990e7152a718aaf",
      "3f3061edddeb41dfb90b9e473a7eb598",
      "15083d3fb24f46f59b20f342ca4ab581",
      "a815d8bfdfec4e7ab36ba258d5672414",
      "3f4d464a24024160afae4df409484902",
      "52a71a7cb9a64fa88ba1cb8bf3d65a30",
      "18c544ca1c11493ea9de1ae193d834f7",
      "5a97c7b1b93d4ed6b3b68b72f0500bf5",
      "7bb11b11c10f4717a23b14e6441aad65",
      "826db38bcad84246aa43678ad3573459",
      "c8b9b2d74702478abc2bdccaace3ea06",
      "27b3adfe606a420ebb5f970953322aad",
      "717fd50e73774fe39efc9e6ce19175eb",
      "cf637dc3c43047028f5733d3da4be581",
      "494682701c8742f88ac89ab57127603f",
      "6e4a7cef7f6f4de6bf52e38119e72563",
      "7334582633c6491393089c23f16a7fe1",
      "0183db9de3ed4f28b4c760ca07eca116",
      "da598829fbc943699c702d76e63b0b09",
      "920beab15c5c42fd9d939362993be305",
      "e1e9527deb2241bfb55f7ee6828def7b",
      "1b7ddd345f444bbfa7ce188ba49d79e7",
      "80f3a3274d074818821697a118761a54",
      "5388f8938c344d788bc4827cf84763b3",
      "96d25862344149b8b8973d74d121519c",
      "d85fe559b95b4fc3b707bd1f71c4fae8",
      "637814982ca449e5b0a79299f7a200db",
      "18515f7b131a4c6c9c213befed90420b",
      "992534a96d5d4f01969676e8a4dd7a7c",
      "9569946a1f4149fbbe119de461e32038",
      "c6aef4b2c9a54a7fa38d80bbd924560f",
      "ac8e1336456e464abec1d2d94a35f167",
      "289b5d653d9a48bf9fe638d41c321ec5",
      "170b7d7963ff49429c9f900228c19e0b",
      "5c99e445b9014dfab6bdbf597f16bcce",
      "686d6525cc7d48d48177c499fd861729",
      "285587120af54fc79d50fa057996c60a",
      "fa7ed7a43c7041a5aa12778638fc2e50",
      "e4230bbf9bc849f09c3e201624cc4f6a",
      "b21ea01153b244cebe465c01647a4a65",
      "a60527755d5545e292adbe437617aa8f",
      "69267aa3550d423a8b8a33058ac24835",
      "0a4da2f639fe40f3812b586d6b60d65a",
      "c14fe0f4fde04d479cdf41e95e71212d",
      "d842bc04f95e4a2797df7e973c70c798",
      "5470c46260614118b04233c9365aa5f8",
      "00434c5e038f48ccb426162fcdd46c0e",
      "ac43e8bacfc64db49a1bfd8855bf0e5e",
      "9ec638ddc9694d0081d8ee2429bafb4c",
      "a6fe36721ff54c669d08c2c9e480c7e9",
      "38de7f104e52435aa74f5c07775989da",
      "35d6802d1c774594b38e0e2cd92e0e1f",
      "1995b5660c384514a00002bc65e0d4eb",
      "1446fb358b794ed78694363e2932f278",
      "5efbf3dfa62c4027beeb5cbea7e1c16c",
      "f167de121d8b4ca4a879b4490970eaca",
      "a197dccf0cae49e4820c8b9de1847b1f",
      "e086e49338744ed2bd6aca31bdbaba77",
      "1c8c0e58d62c443a8988409ee822cf52",
      "fe55095bd20b4d3983c645e0dddb3726",
      "ccff2172d55940fd9b9f3260ed2a2832",
      "44d55520c1b14621af66edf9d3813947",
      "b46be029bea442cd92c9a8e97f07c848",
      "c7b03a1c00344c4ab0509cad5267de06",
      "b5dd943889e04206b9b7e35d57155f6c",
      "477def85abac4728baf5db9cf22322c7",
      "b2cd4405c3ac4fcaa302a710bc6873ab",
      "8f776e160c2e492bbd06574c85a7a2c0",
      "a243a521db2a47aa906ebd9a3127f79a",
      "802d5156494746d3b588a27b7776e005",
      "eed53f871f19498c8c445414b5c02d81"
     ]
    },
    "id": "f6fde15a",
    "outputId": "bee5e5fe-e788-49f3-82a1-8f07659b2437"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37b9ab9478404979928dcc71c1f60c8d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52a71a7cb9a64fa88ba1cb8bf3d65a30"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7334582633c6491393089c23f16a7fe1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18515f7b131a4c6c9c213befed90420b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4230bbf9bc849f09c3e201624cc4f6a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6fe36721ff54c669d08c2c9e480c7e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccff2172d55940fd9b9f3260ed2a2832"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Added tokens: 15\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch, textwrap\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"  # tiny model good for CPT demo\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME, max_seq_length=max_seq_length, dtype=dtype, load_in_4bit=False\n",
    ")\n",
    "\n",
    "# Suppose we want to teach a tiny constructed language (\"MiniLang\")\n",
    "raw_corpus = \"\"\"\n",
    "<doc>\n",
    "mi wile moku. sina pona!\n",
    "tomo li suli. jan li moku e kili.\n",
    "tenpo ni la mi tawa tomo. sina lon seme?\n",
    "</doc>\n",
    "<doc>\n",
    "moku pona li lon kulupu ni. toki sina li pona mute.\n",
    "</doc>\n",
    "\"\"\"\n",
    "\n",
    "# Extend tokenizer if many new tokens\n",
    "added = tokenizer.add_tokens([\"mi\",\"sina\",\"pona\",\"moku\",\"tomo\",\"jan\",\"kili\",\"tenpo\",\"ni\",\"la\",\"tawa\",\"lon\",\"seme\",\"kulupu\",\"toki\"], special_tokens=False)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Added tokens:\", added)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be6dc33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "780aaf30d5e8476bbf3e05411af4387b",
      "2168769817b14d55a02eb40671c59446",
      "f3800572d9c94e9eb719a958a1b2ef8e",
      "1d234a8eae404ea3ba11263a2de24158",
      "68eaa285cff34901a6b3f5e780278884",
      "d754d0299bce4a07a460e7fbbc8f5c91",
      "1a474aa2dccf471f83b3c082cfe1325c",
      "cc0da073cef04a4b9a30eba7d9abcdda",
      "4f40aa757cef4e1095564a465d5749f5",
      "f02dcbbccd054a30b23c6f9c862f6925",
      "748d50a7a88742cda1a3ff815428d977"
     ]
    },
    "id": "6be6dc33",
    "outputId": "e14c5669-79b4-449c-81f7-f119579918a6"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "780aaf30d5e8476bbf3e05411af4387b"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Build raw text dataset for CPT\n",
    "from datasets import Dataset\n",
    "\n",
    "lines = [l.strip() for l in raw_corpus.splitlines() if l.strip()]\n",
    "dataset = Dataset.from_dict({\"text\": lines})\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_raw, eval_raw = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Simple chunking function\n",
    "def pack(examples):\n",
    "    return examples\n",
    "\n",
    "train_raw = train_raw.map(pack, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7875c00e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747,
     "referenced_widgets": [
      "833d6a81ec1b417ebb420a461d4043f6",
      "2da583fd9d594a17998654caef6598e2",
      "5c0fe93d7b2c4166b09581093ff2d1ea",
      "a070c0f9eaa74b67a84b7fddceff76bc",
      "fd09457be2e84e668727c3420d98b3a0",
      "3e463d1ac22a4e3d8eb2e9d91d6f6a0b",
      "d2d1500da42c4db5a6bef068cdce34b6",
      "9e51ab9cc26a478c884d4d17b71d243f",
      "2bfcadb5dee941909fcd5dcd0032aa5e",
      "0cf1a0eb68ec408086595af6528df07a",
      "ed1a888617d24ede9a857816198941a7",
      "a24d6034df594f69be40118c0ea780e3",
      "025dae2073bf48f3a805f7ae76597675",
      "a2d2fa9c4470410082ccb0f8c357d842",
      "dbd1cf5dea96447ea5ec4f922d425de8",
      "941d4152817242f9bef570b2271b5bab",
      "2c3dc30bcb814d379028f490aab3fb13",
      "76f48cfb5dc7438698e9c7c1b1ad6ab0",
      "5c7804206b34483997648ed97ff8137f",
      "e6dc29ea844e4f319e7ed30fbdf37682",
      "db8b05e7776541c0ade4a682aa62a1f7",
      "3357961005f24f05b1d2b7d7759154c8"
     ]
    },
    "id": "7875c00e",
    "outputId": "608bf85e-d253-41bd-9c26-550dd0fb68a3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 7. Reducing num_proc to 7 for dataset of size 7.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=7):   0%|          | 0/7 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "833d6a81ec1b417ebb420a461d4043f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "WARNING:datasets.arrow_dataset:num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=1):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a24d6034df594f69be40118c0ea780e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7 | Num Epochs = 3 | Total steps = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 106,203,456 of 134,520,768 (78.95% trained)\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamipn\u001b[0m (\u001b[33msamipn-san-jose-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251110_015247-gia9kjre</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samipn-san-jose-state-university/huggingface/runs/gia9kjre' target=\"_blank\">dry-aardvark-6</a></strong> to <a href='https://wandb.ai/samipn-san-jose-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/samipn-san-jose-state-university/huggingface' target=\"_blank\">https://wandb.ai/samipn-san-jose-state-university/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/samipn-san-jose-state-university/huggingface/runs/gia9kjre' target=\"_blank\">https://wandb.ai/samipn-san-jose-state-university/huggingface/runs/gia9kjre</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('smollm2_cpt/tokenizer_config.json',\n",
       " 'smollm2_cpt/special_tokens_map.json',\n",
       " 'smollm2_cpt/vocab.json',\n",
       " 'smollm2_cpt/merges.txt',\n",
       " 'smollm2_cpt/added_tokens.json',\n",
       " 'smollm2_cpt/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Train with TRL SFTTrainer in plain text mode (text completion)\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"outputs_cpt_smollm2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    max_seq_length=512,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_raw,\n",
    "    eval_dataset=eval_raw,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=cfg,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"smollm2_cpt\")\n",
    "tokenizer.save_pretrained(\"smollm2_cpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b84787",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85b84787",
    "outputId": "4bb15940-31b6-418b-baa5-61b115fbfc61"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No prior adapter found in this session; skip resume demo.\n"
     ]
    }
   ],
   "source": [
    "# Resume from a LoRA adapter (continued finetuning)\n",
    "# Demonstrate loading a prior adapter and resuming (if available).\n",
    "from pathlib import Path\n",
    "adapter_dir = Path(\"smollm2_lora_adapter\")\n",
    "if adapter_dir.exists():\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir.as_posix())\n",
    "    print(\"Loaded previous LoRA adapter; you can trainer.train() again to continue.\")\n",
    "else:\n",
    "    print(\"No prior adapter found in this session; skip resume demo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f761226",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f761226",
    "outputId": "93ecb9ab-5f41-40f7-bbce-52148254c25f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Merge skipped: type object 'FastLanguageModel' has no attribute 'merge_and_unload'\n"
     ]
    }
   ],
   "source": [
    "# Export to Ollama (auto Modelfile) after merge to 16-bit\n",
    "from unsloth import FastLanguageModel as FLM\n",
    "try:\n",
    "    merged = FLM.merge_and_unload(model)\n",
    "    merged.save_pretrained(\"smollm2_cpt_merged_16bit\")\n",
    "    tokenizer.save_pretrained(\"smollm2_cpt_merged_16bit\")\n",
    "    print(\"Merged model ready. To create Modelfile in Python, see Unsloth docs.\")\n",
    "except Exception as e:\n",
    "    print(\"Merge skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3cd9de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc3cd9de",
    "outputId": "ad2df7d8-0438-49dc-ddeb-77dcb5b6d90b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "toki: sina pona la mi pona. o toki e ijo.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o  elegans  elegans.  o \n"
     ]
    }
   ],
   "source": [
    "# Minimal inference in the new language (qualitative)\n",
    "from unsloth import FastLanguageModel\n",
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"toki: sina pona la mi pona. o toki e ijo.\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/samipn/unsloth.ai_demo/blob/main/colab4_grpo_gsm8k_gemma1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99668453",
   "metadata": {
    "id": "99668453"
   },
   "source": [
    "# Colab 4 â€” GRPO (self-play reasoning)\n",
    "Train a reasoning model with **GRPO** on GSM8K prompts. Model: `unsloth/gemma-3-1b-it-bnb-4bit`. We define rewards for format and final-answer accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55399c62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55399c62",
    "outputId": "844b6eae-23d9-4d37-c091-ae33be1b1ca7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch: 2.8.0+cu126 CUDA: 12.6 Python: 3.12.12\n"
     ]
    }
   ],
   "source": [
    "#@title Install Unsloth + deps (Colab-safe)\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install unsloth datasets trl transformers accelerate bitsandbytes peft --no-cache-dir\n",
    "import torch, platform\n",
    "print(\"PyTorch:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"Python:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28334967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "974a3c8f247242999ea725567080d45a",
      "a48c4c746e274c378fe641a27b9c44ad",
      "3b1f2ad8c4b14432887d51681e0d1742",
      "adcbb0e4a28e43fca3297e49929f9a90",
      "15658004962e45888c93cfe348d608c5",
      "de6c720d86404ab699e05ac02f8a499e",
      "40ffe778d4f64b39a017f501128b6ac7",
      "4181f4879c5c4b7dbb9b65a0f92d7dfe",
      "38c2750dc3ee430c8a968ca51670c1d8",
      "a7dd237bb35c46d19376ceaa7c820ac9",
      "50507d1f988f4e05b6d3998294616545",
      "bfd45a78452f44259b81d63dde23dbc4",
      "1f6581f7481e4fd8aa80201adbba0096",
      "e702d585212643ae8fd4903b91c99d1b",
      "a20b0e6bf4c04a8e8262aa55e3acb7a9",
      "335fdc3e1bfd4e7483ccba87f9c116f3",
      "1308c26253b24553b8252cfe9392172d",
      "18b105c91fad44eca82d9eb21fecc328",
      "aaffdfa1bcd747f2a12c2917c4cbb8c5",
      "321da5beac4d413bb83e16982fa61625",
      "c296e02cfef8498497dc2975ea6178ee",
      "c41119543563402db5c74934de6d0f2b",
      "93f19ac733d54b5bb67a573ca45ddca9",
      "06c5a6c99b27471bb458e3f3cc7308fb",
      "d580ecce87734ef18666df60c343e47b",
      "6bc2ea50557546e8a39090fa5f973c49",
      "b37c51cca5c049b2bb8f51a6cc2a13d1",
      "b3ff04110a94448fad8508f515f36f44",
      "96c6704fd9da4692924e9445af18880e",
      "c9bbb0f4d61a4365a117341fdd557d1c",
      "0f49095af74a478c8bbefbc89fa4d4a7",
      "5efd50bbd6634f08a4e288bb13ea3ab0",
      "c7c7148a2d8145f2b543e5c9ada3bd61",
      "30240f964eae436a8b0afe5ae68a0913",
      "cecf679ee546477983b9e488e31b6a9a",
      "a682e45221ea46778837031b92ff3367",
      "b65a7059714e4efba22adba073701971",
      "17e7ceda93844e788e6fb6d1f6d57a95",
      "34111afb415d4add9c7f23ff439d9832",
      "6972470058704863b986cc237f911690",
      "4da9a7c2ed75483b9fb929852c0a7b08",
      "c3e97c7b95ef45a493ceee0a8b908df1",
      "9594b039fc6746ddb47f90c624acf802",
      "9c182b81e6ae47bd8e70de64bfa103d7",
      "45a5cced677844cdbd90e9d933379fba",
      "6b7e8d3820ac4fe793fac7654eba03c3",
      "2d65f772e2064c1290d3d6d7d5f7cc59",
      "f9f9c40b81624d0a921429821b858b0a",
      "f2d46d4511c4461482916f35443513ae",
      "ac9ddc4c23d34cdabfaf3b89f62079ba",
      "46c62f1fe5124cf2804f478e4dfb83f6",
      "26bc7a4a7ef24adca33a729f9ff8c039",
      "233fb02db1244574a20ad2b97a9d7264",
      "40e381f30231451fb9e16c06387ceed0",
      "dbe77d01070c4d0ab51612728ff6d47f",
      "15f29de4951d4f17b15462a0b0db99a5",
      "27debb5043ae42a686337e5ab5d32187",
      "15bf8d7c6b7f46b685930093be4e338e",
      "5008eafd05664e03af0b4e0735760a16",
      "8aa5ed801872437da14c2f78caedad68",
      "fd90b6f7cd164d79a242bb72ecaad3c8",
      "03bbc6e40e9a45d8be53aa691aedf4f7",
      "658adb073fda43ad8763bd855621d120",
      "b4b668eb0c6c4a8889dd1e50eef8e471",
      "d9238815460947cda143a76e4a578332",
      "2b7302529e83400c973853345568fd45",
      "f41fc6fce007450dac181cf3bcd3220c",
      "29578dd543594ce7ba3c85419bf66770",
      "57cc7c8f012f4c03864dec08d7961cf5",
      "b7bfdd72af9648a9ac1fe6325390cf2e",
      "f9d11e4058224191a9f1cb6b9ce4dd7e",
      "61a26ca0fd304ddfbfe1c34b2db72831",
      "eeb016d266664de7beb170d868f9593e",
      "763ae839dbd04342be9e2afce7388599",
      "d66b29b41e034807a70310822abd29a7",
      "1a368a7c94c34f2cb3f9d92f90f7d967",
      "392157f98d844967bd15351ae3194d60"
     ]
    },
    "id": "28334967",
    "outputId": "719777ce-690b-48b5-8f5f-9079c4629cdd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Gemma3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/965M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "974a3c8f247242999ea725567080d45a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfd45a78452f44259b81d63dde23dbc4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93f19ac733d54b5bb67a573ca45ddca9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30240f964eae436a8b0afe5ae68a0913"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45a5cced677844cdbd90e9d933379fba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15f29de4951d4f17b15462a0b0db99a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f41fc6fce007450dac181cf3bcd3220c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"unsloth/gemma-3-1b-it-bnb-4bit\"\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, lora_alpha=16, lora_dropout=0.0,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407, max_seq_length=max_seq_length,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "416e5152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ce8057b5679f4050bac6a5ea2c89ed45",
      "2aed0376f1184e72a983c24f159c327d",
      "5d1774b2a30c4afda7fd2840a9cbb769",
      "296c3222b4cb49c997d6dc1779e7c64c",
      "4a9661c823884e9bb1ebeac87bbfb2da",
      "f08d24abfd2642679869adacd1a6a416",
      "24801b9030d2453e8c9d83354c8031d6",
      "5ff2c519c4ec40da9c933af0939cebc8",
      "01cc5ef2fb324ec69a18e3c6f5e0ca6d",
      "d586d54614984d8595613b2383343ca7",
      "3642459eee1240728c0b4461d80759be"
     ]
    },
    "id": "416e5152",
    "outputId": "c7731797-e66e-4b74-ec22-287c92530f52"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce8057b5679f4050bac6a5ea2c89ed45"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Prepare GSM8K prompts\n",
    "from datasets import load_dataset\n",
    "\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train[:200]\")  # small slice for speed\n",
    "\n",
    "SYSTEM = \"You are a step-by-step math tutor. Think aloud inside <reasoning> tags, then give the final boxed answer in <answer> tags.\"\n",
    "def to_prompt(ex):\n",
    "    q = ex[\"question\"].strip()\n",
    "    messages = [{\"role\":\"system\",\"content\":SYSTEM},{\"role\":\"user\",\"content\":q}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # we also keep ground truth answer for reward\n",
    "    return {\"prompt\": prompt, \"solution\": ex[\"answer\"]}\n",
    "\n",
    "ds = gsm8k.map(to_prompt, remove_columns=gsm8k.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5c5fae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "9e5c5fae",
    "outputId": "f8f2a2d1-9000-44db-bce9-5e61c95572b2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 200 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:02:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / reward_fn / mean</th>\n",
       "      <th>rewards / reward_fn / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>245.975000</td>\n",
       "      <td>226.500000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>132.050000</td>\n",
       "      <td>124.100000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.900000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>255.200000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>164.783336</td>\n",
       "      <td>151.800000</td>\n",
       "      <td>174.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>243.525000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>147.600000</td>\n",
       "      <td>143.700000</td>\n",
       "      <td>151.500000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.875000</td>\n",
       "      <td>205.500000</td>\n",
       "      <td>255.700000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>144.941669</td>\n",
       "      <td>128.700000</td>\n",
       "      <td>155.400000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>239.075000</td>\n",
       "      <td>217.600000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>121.233334</td>\n",
       "      <td>115.200000</td>\n",
       "      <td>127.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.150000</td>\n",
       "      <td>219.500000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>122.100000</td>\n",
       "      <td>117.100000</td>\n",
       "      <td>126.600000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>236.450000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>191.866667</td>\n",
       "      <td>179.400000</td>\n",
       "      <td>204.200000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>248.100000</td>\n",
       "      <td>230.600000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>136.366667</td>\n",
       "      <td>128.200000</td>\n",
       "      <td>144.500000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>207.400000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>134.850000</td>\n",
       "      <td>130.600000</td>\n",
       "      <td>139.100000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.425000</td>\n",
       "      <td>195.900000</td>\n",
       "      <td>255.700000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>157.500000</td>\n",
       "      <td>144.700000</td>\n",
       "      <td>166.800000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('gemma1b_grpo_adapter/tokenizer_config.json',\n",
       " 'gemma1b_grpo_adapter/special_tokens_map.json',\n",
       " 'gemma1b_grpo_adapter/chat_template.jinja',\n",
       " 'gemma1b_grpo_adapter/tokenizer.model',\n",
       " 'gemma1b_grpo_adapter/added_tokens.json',\n",
       " 'gemma1b_grpo_adapter/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "import re, torch\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "def extract_answer(text):\n",
    "    # look for \\boxed{...} or <answer>...</answer>\n",
    "    m = re.search(r\"\\\\boxed\\{([^}]+)\\}|<answer>(.*?)</answer>\", text, flags=re.S|re.I)\n",
    "    if not m: return None\n",
    "    return m.group(1) if m.group(1) is not None else m.group(2)\n",
    "\n",
    "def format_reward(text):\n",
    "    return 1.0 if (\"<reasoning>\" in text and \"</reasoning>\" in text and \"<answer>\" in text and \"</answer>\" in text) else 0.0\n",
    "\n",
    "def accuracy_reward(text, target):\n",
    "    pred = extract_answer(text)\n",
    "    # crude normalization (remove punctuation/spaces)\n",
    "    if pred is None: return 0.0\n",
    "    P = re.sub(r\"[^0-9.-]\", \"\", pred)\n",
    "    T = re.sub(r\"[^0-9.-]\", \"\", target)\n",
    "    return 1.0 if P == T and P != \"\" else 0.0\n",
    "\n",
    "def reward_fn(prompts, completions, completion_ids, solution, trainer_state):\n",
    "    # completions: list[str] are the generated samples\n",
    "    # prompts: list[str] are the original prompts\n",
    "    # completion_ids: list[torch.Tensor] are the tokenized completion ids\n",
    "    # solution: list[str] are the ground truth answers from info_column\n",
    "    # trainer_state: dict containing information about the current training state (e.g., global_step)\n",
    "    rewards = []\n",
    "    for c, s in zip(completions, solution):\n",
    "        r = 0.6 * format_reward(c) + 0.4 * accuracy_reward(c, s)\n",
    "        rewards.append(r)\n",
    "    return rewards\n",
    "\n",
    "cfg = GRPOConfig(\n",
    "    output_dir=\"outputs_grpo_gemma1b\",\n",
    "    per_device_train_batch_size=1,   # GRPO samples multiple candidates / step\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    max_completion_length=256,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    num_generations=2,   # group size\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds,\n",
    "    reward_funcs=[reward_fn],\n",
    "    args=cfg,\n",
    "    dataset_num_proc=1,\n",
    "    prompt_column=\"prompt\",\n",
    "    info_column=\"solution\",\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"gemma1b_grpo_adapter\")\n",
    "tokenizer.save_pretrained(\"gemma1b_grpo_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c08c58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62c08c58",
    "outputId": "c97d94ce-4a10-4a39-cbd5-645da3eaea7b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Say hi in one sentence.\n",
      "model\n",
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Quick inference helper\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # enables 2x faster kernels (no change to outputs)\n",
    "\n",
    "def chat(prompt, history=None, max_new_tokens=128):\n",
    "    if history is None: history = []\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "chat(\"Say hi in one sentence.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
